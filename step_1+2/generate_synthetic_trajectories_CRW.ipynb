{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "#\n",
    "import numpy as np\n",
    "from numpy import log, mean, sqrt, where, std, exp, sign\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import truncnorm\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import vonmises\n",
    "#\n",
    "import seaborn as sns\n",
    "#\n",
    "# https://stochastic.readthedocs.io/en/latest/general.html\n",
    "# \n",
    "from fbm import FBM\n",
    "from stochastic.processes import OrnsteinUhlenbeckProcess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc77173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import finfo\n",
    "epsilon = finfo(float).eps\n",
    "print(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# basic functions\n",
    "#\n",
    "def get_displacement(x_1, y_1, x_2, y_2):\n",
    "    # compute the displacement between two points\n",
    "    return sqrt((x_1-x_2)**2+(y_1-y_2)**2)\n",
    "\n",
    "def get_displacements(x, y):\n",
    "    #\n",
    "    # inputs\n",
    "    # x: list of x coordinate\n",
    "    # y: list of y coordinate\n",
    "    #\n",
    "    # results\n",
    "    # displacements: list of displacements between position at t(i-1) and t(i)\n",
    "    #\n",
    "    n_datapoints = len(x)\n",
    "    displacements = np.array([get_displacement(x[i-1], y[i-1], x[i], y[i]) for i in range(1, n_datapoints)])\n",
    "    return displacements\n",
    "\n",
    "def get_d_traveled(x, y):\n",
    "    #\n",
    "    # inputs\n",
    "    # x: list of x coordinate\n",
    "    # y: list of y coordinate\n",
    "    #\n",
    "    # results\n",
    "    # d_traveled: sum of list of displacements\n",
    "    #    \n",
    "    n_datapoints = len(x)\n",
    "    d_traveled = sum(get_displacements(x, y)) \n",
    "    return d_traveled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeca287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# feature : efficiency and straightness\n",
    "#\n",
    "def get_efficiency(x, y):\n",
    "    #\n",
    "    # inputs\n",
    "    # traj_x: list of x coordinate\n",
    "    # traj_y: list of y coordinate\n",
    "    #\n",
    "    # results\n",
    "    # efficiency: relevant to trajectory linearity; may be useful for detecting directed motion\n",
    "    #       \n",
    "    n_datapoints = len(x)\n",
    "    upper = get_displacement(x[n_datapoints-1], y[n_datapoints-1], x[0], y[0])**2\n",
    "    displacements_sq = get_displacements(x, y)**2\n",
    "    lower = (n_datapoints-1)*sum(displacements_sq)\n",
    "    efficiency = upper/lower\n",
    "    return efficiency\n",
    "\n",
    "def get_straightness(x, y):\n",
    "    #\n",
    "    # inputs\n",
    "    # x: list of x coordinate\n",
    "    # y: list of y coordinate\n",
    "    #\n",
    "    # results\n",
    "    # straightness: relevant to average direction change between subsequent steps; may be useful for detecting directed motion\n",
    "    #         \n",
    "    n_datapoints = len(x)\n",
    "    x_0 = x[0]\n",
    "    y_0 = y[0]\n",
    "    x_f = x[n_datapoints-1]\n",
    "    y_f = y[n_datapoints-1]     \n",
    "    upper = get_displacement(x_0, y_0, x_f, y_f)\n",
    "    lower = get_d_traveled(x, y)\n",
    "    straightness = upper/lower\n",
    "    return straightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# for computing the ratio of net displacement and distance traveled\n",
    "# see: \n",
    "# Huang, S.Y., Zou, X.W. and Jin, Z.Z. (2002) Directed random walks in continuous space. Physical Review E, 65, 052105.\n",
    "# Visser, A.W. and KiÃ¸rboe, T. (2006) Plankton motility patterns and encounter rates. Oecologia, 148, pp. 538--546.\n",
    "#\n",
    "\n",
    "def get_net_displacement(traj_x, traj_y):\n",
    "    #\n",
    "    # inputs\n",
    "    # traj_x: collection of list of x coordinate\n",
    "    # traj_y: collection of list of y coordinate\n",
    "    #\n",
    "    # results  \n",
    "    # d_net_set: list of net displacement\n",
    "    #\n",
    "    n_runs = len(traj_x)\n",
    "    d_net_set = []\n",
    "    for i in range(0, n_runs):\n",
    "        n_datapoints = len(traj_x[i])\n",
    "        x_0 = traj_x[i][0]\n",
    "        y_0 = traj_y[i][0]\n",
    "        x_f = traj_x[i][n_datapoints-1]\n",
    "        y_f = traj_y[i][n_datapoints-1]        \n",
    "        d_net = get_displacement(x_0, y_0, x_f, y_f)\n",
    "        d_net_set.append(d_net)\n",
    "    return d_net_set\n",
    "\n",
    "def get_pathlength(traj_x, traj_y):\n",
    "    #\n",
    "    # inputs\n",
    "    # traj_x: collection of list of x coordinate\n",
    "    # traj_y: collection of list of y coordinate\n",
    "    #\n",
    "    # results  \n",
    "    # d_traveled_set: list of distance traveled\n",
    "    #    \n",
    "    n_runs = len(traj_x)\n",
    "    d_traveled_set = []\n",
    "    for i in range(0, n_runs):\n",
    "        d_traveled = get_d_traveled(traj_x[i], traj_y[i])\n",
    "        d_traveled_set.append(d_traveled)\n",
    "    return d_traveled_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1030cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature: \n",
    "# smallest enclosing circle (i.e., minimum bounding circle) and its normalized value\n",
    "# normalized against distance traveled\n",
    "#\n",
    "def mbc_xyr(traj_x, traj_y):\n",
    "    # note that traj_x and traj_y contain multiple records of x and y cordinates\n",
    "    # size of traj_x and traj_y is n_trajectories\n",
    "    n_runs = len(traj_x)\n",
    "    #\n",
    "    mbc_x = []\n",
    "    mbc_y = []\n",
    "    mbc_r = []\n",
    "    nmbcd = []\n",
    "    #\n",
    "    for i in range(0, n_runs):\n",
    "        position = list(zip(traj_x[i], traj_y[i]))\n",
    "        mbcx_i, mbcy_i, mbcr_i = smallestenclosingcircle.make_circle(position)\n",
    "        d_traveled = get_d_traveled(traj_x[i], traj_y[i])\n",
    "        nmbcd_i = 2*mbcr_i/d_traveled # normalized minimum bounding circle size\n",
    "        mbc_x.append(mbcx_i)\n",
    "        mbc_y.append(mbcy_i)\n",
    "        mbc_r.append(mbcr_i)\n",
    "        nmbcd.append(nmbcd_i) \n",
    "    #\n",
    "    # sort elements from lower to higher\n",
    "    mbc_x = np.sort(mbc_x) \n",
    "    mbc_y = np.sort(mbc_y) \n",
    "    mbc_r = np.sort(mbc_r) \n",
    "    nmbcd = np.sort(nmbcd) \n",
    "    #\n",
    "    return mbc_x, mbc_y, mbc_r, nmbcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# feature : turning angle \n",
    "#\n",
    "# entropy of turning angle \n",
    "# see:\n",
    "# Liu et al. PRE 2017\n",
    "# Establishing the kinetics of ballistic-to-diffusive transition using directional statistics\n",
    "# Appendix A: Determining theta from trajectory\n",
    "#\n",
    "def get_turning_angle_measures(traj_x, traj_y, tau_frame, binwidth):\n",
    "    #\n",
    "    # inputs\n",
    "    # traj_x = record_raw['relative_x']: list of x coordinate\n",
    "    # traj_y = record_raw['relative_y']: list of y coordinate  \n",
    "    # tau_frame: relevant to sampling frequency\n",
    "    # binwidth: bin width for computing entropy\n",
    "    #\n",
    "    # results\n",
    "    # turning_angle_avg: average of turning angle\n",
    "    # turning_angle_std: standard deviation of turning angle\n",
    "    # turning_angle_entropy: Shannon entropy of turning angle\n",
    "    #\n",
    "    # compute turning angle\n",
    "    #    \n",
    "    traj_x = pd.Series(traj_x)\n",
    "    traj_y = pd.Series(traj_y)\n",
    "    relative_x = traj_x[::tau_frame] # every tau-th row\n",
    "    relative_y = traj_y[::tau_frame] # every tau-th row  \n",
    "    relative_x = relative_x.reset_index(drop=True) # reset row index\n",
    "    relative_y = relative_y.reset_index(drop=True) # reset row index\n",
    "    turning_angle = []\n",
    "    for i in range (1, len(relative_x)-1):\n",
    "        diff_x1 = relative_x[i]-relative_x[i-1]\n",
    "        diff_x2 = relative_x[i+1]-relative_x[i]\n",
    "        diff_y1 = relative_y[i]-relative_y[i-1]\n",
    "        diff_y2 = relative_y[i+1]-relative_y[i] \n",
    "        # compute k1 and k2\n",
    "        k1 = 0 # diff_x1 > 0 and diff_y1 > 0\n",
    "        if (diff_x1 >= 0):\n",
    "            if (diff_y1 >= 0):\n",
    "                k1 = 0\n",
    "            else:\n",
    "                k1 = 2\n",
    "        if (diff_x1 < 0):\n",
    "            k1 = 1\n",
    "        k2 = 0\n",
    "        if (diff_x2 >= 0):\n",
    "            if (diff_y2 >= 0):\n",
    "                k2 = 0\n",
    "            else:\n",
    "                k2 = 2\n",
    "        if (diff_x2 < 0):\n",
    "            k2 = 1    \n",
    "        # compute phi_1 and phi_2\n",
    "        # arc tangent of y/x in radians\n",
    "        phi_1 = k1*np.pi + math.atan2(diff_y1, diff_x1) # 0, ..., 2*np.pi\n",
    "        phi_2 = k2*np.pi + math.atan2(diff_y2, diff_x2) # 0, ..., 2*np.pi\n",
    "        # compute m\n",
    "        m = 0\n",
    "        phi_diff = abs(phi_2-phi_1)\n",
    "        if (phi_diff < np.pi):\n",
    "            m = 0\n",
    "        if (phi_diff > np.pi):\n",
    "            if (phi_2 > phi_1):\n",
    "                m = -1\n",
    "            if (phi_2 < phi_1):\n",
    "                m = 1\n",
    "        # compute theta\n",
    "        theta_i = 2*m*np.pi+phi_2-phi_1  # -np.pi, ..., np.pi\n",
    "        turning_angle.append(theta_i)\n",
    "    #\n",
    "    # compute entropy\n",
    "    #\n",
    "    epsilon_p = 0.0001\n",
    "    epsilon_q = 0.0001\n",
    "    # create relative frequency histogram\n",
    "    turning_angle_deg = [(x/np.pi)*180.0 for x in turning_angle] # degree\n",
    "    x_max = 180 # deg\n",
    "    x_min = -180 # deg\n",
    "    #\n",
    "    bin_list = np.arange(x_min, x_max, binwidth) \n",
    "    hist_p, edges_p = np.histogram(turning_angle_deg, bins=bin_list)\n",
    "    freq_p = hist_p/float(hist_p.sum())\n",
    "    freq_p += epsilon_p\n",
    "    pk = np.reshape(freq_p, -1)\n",
    "    base = len(bin_list) # normalized entropy\n",
    "    entropy_p = entropy(pk, base=base)        \n",
    "    #\n",
    "    turning_angle_avg = np.mean(turning_angle)\n",
    "    turning_angle_std = np.std(turning_angle)\n",
    "    turning_angle_entropy = entropy_p\n",
    "    #\n",
    "    return turning_angle_avg, turning_angle_std, turning_angle_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf486a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_synthetic_trajectory_statistics(x, y, dt, tau_frame):\n",
    "    binwidth_turning_angle = 15 # deg\n",
    "    # tunring angle\n",
    "    angle_avg = []\n",
    "    angle_std = []\n",
    "    angle_entropy = []\n",
    "    # efficiency and straightness\n",
    "    efficiency = []\n",
    "    straightness = []\n",
    "    #\n",
    "    n_trajectories = len(x)\n",
    "    n_steps = len(x[0])\n",
    "    #\n",
    "    for i in range(0, n_trajectories):\n",
    "        x_i = x[i]\n",
    "        y_i = y[i]\n",
    "        # tunring angle\n",
    "        angle_avg_i, angle_std_i, angle_entropy_i = get_turning_angle_measures(x_i, y_i, tau_frame, binwidth_turning_angle)\n",
    "        angle_avg.append(angle_avg_i)\n",
    "        angle_std.append(angle_std_i)\n",
    "        angle_entropy.append(angle_entropy_i)    \n",
    "        # efficiency and straightness\n",
    "        efficiency_i = get_efficiency(x_i, y_i)\n",
    "        efficiency.append(efficiency_i)\n",
    "        straightness_i = get_straightness(x_i, y_i)\n",
    "        straightness.append(straightness_i)\n",
    "    # sort elements from lower to higher\n",
    "    angle_avg = np.sort(angle_avg) \n",
    "    angle_std = np.sort(angle_std) \n",
    "    angle_entropy = np.sort(angle_entropy) \n",
    "    efficiency = np.sort(efficiency) \n",
    "    straightness = np.sort(straightness) \n",
    "    #\n",
    "    return angle_avg, angle_std, angle_entropy, efficiency, straightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graphs(subplot_title, trajectory_x, trajectory_y, variable_x, label_x, x_binwidth, x_min_i, x_max_i, xtick_space):\n",
    "    #\n",
    "    # prepare subplots\n",
    "    #\n",
    "    n_columns = len(variable_x)+1\n",
    "    n_rows = 1\n",
    "    #\n",
    "    subplot_x = 4.0\n",
    "    subplot_y = 3.0\n",
    "    figsize_x = subplot_x*n_columns\n",
    "    figsize_y = subplot_y*n_rows\n",
    "    fig = plt.figure(figsize=(figsize_x, figsize_y))       \n",
    "    #\n",
    "    for i in range (0, n_rows):\n",
    "        #\n",
    "        for j in range (0, n_columns):\n",
    "            ax = fig.add_subplot(n_rows, n_columns, (n_columns*i+j+1))\n",
    "            #\n",
    "            index_ij = i*n_columns+j\n",
    "            #\n",
    "            if (j < n_columns-1):\n",
    "                x_ij = variable_x[index_ij]\n",
    "                #\n",
    "                check_x_min = False\n",
    "                check_x_max = False\n",
    "                if not x_min_i[j]:\n",
    "                    check_x_min = True\n",
    "                if not x_max_i[j]:\n",
    "                    check_x_max = True\n",
    "                #\n",
    "                x_min = 0.0\n",
    "                x_max = 0.0\n",
    "                if (check_x_min == True):\n",
    "                    x_min = min(min(x_ij), 0.0)  \n",
    "                else: \n",
    "                    x_min = x_min_i[j]\n",
    "                if (check_x_max == True):\n",
    "                    x_max = max(x_ij)  \n",
    "                else: \n",
    "                    x_max = x_max_i[j]            \n",
    "                #\n",
    "                n_xticks = 5            \n",
    "                #\n",
    "                bin_list = np.arange(x_min, x_max*1.05, x_binwidth[j]) \n",
    "                counts = plt.hist(x_ij, bins=bin_list, alpha=0.5, weights=np.ones_like(x_ij)/len(x_ij)) # alpha controls transparency\n",
    "                #\n",
    "                plt.xlim(x_min, x_max)\n",
    "                if not xtick_space:\n",
    "                    ax.xaxis.set_major_locator(plt.MaxNLocator(n_xticks))\n",
    "                else:\n",
    "                    plt.xticks(np.arange(x_min, x_max*1.05, xtick_space[j]))       \n",
    "                plt.tick_params(labelsize=12)\n",
    "                #\n",
    "                plt.xlabel(label_x[j], fontsize=12)\n",
    "            if (j == n_columns-1):\n",
    "                plt.plot(trajectory_x[0], trajectory_y[0], color = 'b', lw=3) \n",
    "                plt.xlabel(\"position x (m)\", fontsize=12)\n",
    "                plt.ylabel(\"position y (m)\", fontsize=12)                \n",
    "            if (j == 0):\n",
    "                plt.ylabel('relative frequency', fontsize=12) \n",
    "                plt.title(subplot_title, fontsize=16)               \n",
    "    # set the spacing between subplots\n",
    "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cdf(subplot_title, alpha_significance, estimate_lower, estimate_upper, trajectory_x, trajectory_y, variable_x, label_x, x_binwidth, x_min_i, x_max_i, xtick_space):\n",
    "    #\n",
    "    # prepare subplots\n",
    "    #\n",
    "    n_columns = len(variable_x)+1\n",
    "    n_rows = 1\n",
    "    #\n",
    "    subplot_x = 4.0\n",
    "    subplot_y = 3.0\n",
    "    figsize_x = subplot_x*n_columns\n",
    "    figsize_y = subplot_y*n_rows\n",
    "    fig = plt.figure(figsize=(figsize_x, figsize_y))       \n",
    "    #\n",
    "    for i in range (0, n_rows):\n",
    "        #\n",
    "        for j in range (0, n_columns):\n",
    "            ax = fig.add_subplot(n_rows, n_columns, (n_columns*i+j+1))\n",
    "            #\n",
    "            index_ij = i*n_columns+j\n",
    "            #\n",
    "            if (j < n_columns-1):\n",
    "                x_ij = variable_x[index_ij]\n",
    "                #\n",
    "                check_x_min = False\n",
    "                check_x_max = False\n",
    "                if not x_min_i[j]:\n",
    "                    check_x_min = True\n",
    "                if not x_max_i[j]:\n",
    "                    check_x_max = True\n",
    "                #\n",
    "                x_min = 0.0\n",
    "                x_max = 0.0\n",
    "                if (check_x_min == True):\n",
    "                    x_min = min(min(x_ij), 0.0)  \n",
    "                else: \n",
    "                    x_min = x_min_i[j]\n",
    "                if (check_x_max == True):\n",
    "                    x_max = max(x_ij)  \n",
    "                else: \n",
    "                    x_max = x_max_i[j]            \n",
    "                #\n",
    "                n_xticks = 5            \n",
    "                #\n",
    "                bin_list = np.arange(x_min, x_max*1.05, x_binwidth[j]) \n",
    "                n, bins, patches = plt.hist(x_ij, bins=bin_list, density=True, histtype=\"step\", cumulative=True, lw=3) # cumulative histogram\n",
    "                #\n",
    "                plt.xlim(x_min, x_max)\n",
    "                if not xtick_space:\n",
    "                    ax.xaxis.set_major_locator(plt.MaxNLocator(n_xticks))\n",
    "                else:\n",
    "                    plt.xticks(np.arange(x_min, x_max*1.05, xtick_space[j]))       \n",
    "                plt.tick_params(labelsize=12)\n",
    "                #\n",
    "                alpha = alpha_significance # significance level\n",
    "                # estimate_lower = [index_lower, entropy_lower, efficiency_lower, straightness_lower]\n",
    "                # estimate_upper = [index_upper, entropy_upper, efficiency_upper, straightness_upper]\n",
    "                # x_sorted = np.sort(x_ij) \n",
    "                # n_trajectories = len(x_sorted)\n",
    "                # index_lower = int(n_trajectories*alpha*0.5)\n",
    "                # index_upper = n_trajectories-index_lower\n",
    "                # x_left  = x_sorted[index_lower]\n",
    "                # x_right = x_sorted[index_upper]\n",
    "                #\n",
    "                x_left  = estimate_lower[j+1]\n",
    "                x_right = estimate_upper[j+1]                \n",
    "                plt.axvspan(x_left, x_right, color='r', alpha=0.5, lw=0)\n",
    "                # plt.axhline(y = 0.5*alpha, color = 'r', linestyle = 'dashed', label = \"red line\")    \n",
    "                # plt.axhline(y = 1.0-0.5*alpha, color = 'r', linestyle = 'dashed', label = \"red line\")    \n",
    "                #\n",
    "                plt.xlabel(label_x[j], fontsize=12)  \n",
    "            if (j == n_columns-1):\n",
    "                plt.axis('off')\n",
    "            if (j == 0):\n",
    "                plt.ylabel(\"cdf\", fontsize=12) \n",
    "                plt.title(subplot_title, fontsize=16)               \n",
    "    # set the spacing between subplots\n",
    "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_trajectories_vonMises(n_trajectories, n_steps, dt, kappa):\n",
    "    x = [] # trajectory_x\n",
    "    y = [] # trajectory_y\n",
    "    for i in range(0, n_trajectories):\n",
    "        # initialize trajectories\n",
    "        x_i = np.zeros(n_steps, dtype=float)\n",
    "        y_i = np.zeros(n_steps, dtype=float)\n",
    "        theta_i = np.zeros(n_steps, dtype=float)\n",
    "        speed_i = 1.0 # unit step\n",
    "        # generate a synthetic trajectory\n",
    "        for j in range(1, n_steps):\n",
    "            # for each step\n",
    "            theta_i[j] = theta_i[j-1]+np.random.vonmises(0, kappa)\n",
    "            x_i[j] = x_i[j-1]+speed_i*np.cos(theta_i[j])*dt\n",
    "            y_i[j] = y_i[j-1]+speed_i*np.sin(theta_i[j])*dt\n",
    "        #\n",
    "        x.append(x_i)\n",
    "        y.append(y_i)\n",
    "    #\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b849500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_synthetic_trajectories_vonMises(n_trajectories, n_steps, alpha_significance, kappa):\n",
    "    #\n",
    "    dt = 1\n",
    "    tau_frame = 2\n",
    "    D_diffusion = 0\n",
    "    #\n",
    "    # generate synthetic trajectories and then compute entropy, efficiency, and nbmcd\n",
    "    #\n",
    "    trajectory_x, trajectory_y = generate_synthetic_trajectories_vonMises(n_trajectories, n_steps, dt, kappa)\n",
    "    angle_avg, angle_std, angle_entropy, efficiency, straightness = compute_synthetic_trajectory_statistics(trajectory_x, trajectory_y, dt, tau_frame)   \n",
    "    d_net  = get_net_displacement(trajectory_x, trajectory_y)\n",
    "    d_path = get_pathlength(trajectory_x, trajectory_y)\n",
    "    #\n",
    "    # sort elements from lower to higher\n",
    "    #\n",
    "    angle_avg = np.sort(angle_avg) \n",
    "    angle_std = np.sort(angle_std) \n",
    "    angle_entropy = np.sort(angle_entropy) \n",
    "    efficiency = np.sort(efficiency) \n",
    "    straightness = np.sort(straightness)     \n",
    "    #\n",
    "    # compute avgerage and standard deviation\n",
    "    #\n",
    "    entropy_avg      = np.mean(angle_entropy)\n",
    "    entropy_std      = np.std(angle_entropy)\n",
    "    efficiency_avg   = np.mean(efficiency)\n",
    "    efficiency_std   = np.std(efficiency)\n",
    "    straightness_avg = np.mean(straightness)\n",
    "    straightness_std = np.std(straightness)\n",
    "    d_net_avg        = np.mean(d_net)\n",
    "    d_path_avg       = np.mean(d_path)\n",
    "    #\n",
    "    variable_x  = [angle_entropy, efficiency, straightness]\n",
    "    label_x     = [\"turning angle entropy\", \"efficiency\", \"straightness\"]\n",
    "    x_binwidth  = [0.05, 0.05, 0.05]\n",
    "    x_min       = [0.0, 0.0, 0.0]\n",
    "    x_max       = [1.0, 1.0, 1.0]\n",
    "    xtick_space = [0.2, 0.2, 0.2]    \n",
    "    #\n",
    "    # find boundary of significance level\n",
    "    #\n",
    "    # for entropy of ballistic motion, we find upper boundary, \n",
    "    # so we consider entropy smaller than the threshold value as ballistic motion\n",
    "    #\n",
    "    # for efficiency and straightness of ballistic motion, we find lower boundary,\n",
    "    # so we consider efficiency and straightness smaller than the threshold value as ballistic motion\n",
    "    #\n",
    "    index_lower        = int(n_trajectories*alpha_significance*0.5)\n",
    "    index_upper        = n_trajectories-index_lower    \n",
    "    #\n",
    "    entropy_lower      = x_min[0]\n",
    "    entropy_upper      = max(angle_entropy[index_upper-1], x_binwidth[0])\n",
    "    efficiency_lower   = min(efficiency[index_lower], (x_max[1]-x_binwidth[1]))\n",
    "    efficiency_upper   = x_max[1]\n",
    "    straightness_lower = min(straightness[index_lower], (x_max[2]-x_binwidth[2]))\n",
    "    straightness_upper = x_max[2]\n",
    "    #\n",
    "    estimate_lower = [index_lower, entropy_lower, efficiency_lower, straightness_lower]\n",
    "    estimate_upper = [index_upper, entropy_upper, efficiency_upper, straightness_upper]   \n",
    "    #\n",
    "    summary_statistics = [kappa, n_steps,  \n",
    "                          entropy_avg, entropy_std, entropy_lower, entropy_upper,\n",
    "                          efficiency_avg, efficiency_std, efficiency_lower, efficiency_upper,\n",
    "                          straightness_avg, straightness_std, straightness_lower, straightness_upper,\n",
    "                          d_net_avg, d_path_avg]             \n",
    "    #\n",
    "    subplot_title = r\"$\\kappa$=\"+str(kappa)+\", n=\"+str(n_steps)\n",
    "    #\n",
    "    generate_graphs(subplot_title, trajectory_x, trajectory_y, variable_x, label_x, x_binwidth, x_min, x_max, xtick_space)\n",
    "    generate_cdf(subplot_title, alpha_significance, estimate_lower, estimate_upper, trajectory_x, trajectory_y, variable_x, label_x, x_binwidth, x_min, x_max, xtick_space)\n",
    "    #\n",
    "    return summary_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create a list for collecting summary statistics \n",
    "# later we will creat a pandas dataframe for writing the summaries\n",
    "# \n",
    "# see\n",
    "# https://stackoverflow.com/questions/17091769/python-pandas-fill-a-dataframe-row-by-row\n",
    "#\n",
    "summary_statistics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(n_steps_set, kappa, summary_statistics):\n",
    "    t_temp = time.localtime()\n",
    "    t_start = time.strftime(\"%H:%M:%S\", t_temp)\n",
    "    print(t_start)\n",
    "    #\n",
    "    # von Mises process\n",
    "    #\n",
    "    n_trajectories = 10000\n",
    "    alpha_significance = 0.05 # significance level, 0.05\n",
    "    #\n",
    "    for n_steps in n_steps_set:\n",
    "        results_i = test_synthetic_trajectories_vonMises(n_trajectories, n_steps, alpha_significance, kappa)\n",
    "        summary_statistics.append(results_i)\n",
    "    #   \n",
    "    # print the summary statistics on screen\n",
    "    #\n",
    "    header = \"kappa\\tn_steps\"\n",
    "    header += \"\\tentropy_avg\\t_std\\t_lower\\t_upper\"\n",
    "    header += \"\\tefficiency_avg\\t_std\\t_lower\\t_upper\"\n",
    "    header += \"\\tstraightness_avg\\t_std\\t_lower\\t_upper\"\n",
    "    header += \"\\td_net\\td_path\"\n",
    "    print(header)\n",
    "    for i in range(0, len(summary_statistics)):\n",
    "        print_summary = str(summary_statistics[i][0])+\"\\t\"+str(summary_statistics[i][1])\n",
    "        for j in range(2, len(summary_statistics[i])):\n",
    "            print_summary += \"\\t{:.3f}\".format(summary_statistics[i][j])\n",
    "        print(print_summary)\n",
    "    #\n",
    "    #\n",
    "    t_temp = time.localtime()\n",
    "    t_end = time.strftime(\"%H:%M:%S\", t_temp)\n",
    "    print(t_end)    \n",
    "    print(t_start)\n",
    "    #\n",
    "    return summary_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ef52c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_steps_set = [10, 25, 50, 75, 100, 200, 250, 300, 500, 750, 1000]\n",
    "kappa = 0\n",
    "summary_statistics = generate_results(n_steps_set, kappa, summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12252cba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_steps_set = [10, 25, 50, 75, 100, 200, 250, 300, 500, 750, 1000]\n",
    "kappa = 10\n",
    "summary_statistics = generate_results(n_steps_set, kappa, summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e29f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_steps_set = [10, 25, 50, 75, 100, 200, 250, 300, 500, 750, 1000]\n",
    "kappa = 50\n",
    "summary_statistics = generate_results(n_steps_set, kappa, summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d463de6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_steps_set = [10, 25, 50, 75, 100, 200, 250, 300, 500, 750, 1000]\n",
    "kappa = 100\n",
    "summary_statistics = generate_results(n_steps_set, kappa, summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244eaf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_steps_set = [10, 25, 50, 75, 100, 200, 250, 300, 500, 750, 1000]\n",
    "kappa = 200\n",
    "summary_statistics = generate_results(n_steps_set, kappa, summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf0c50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_steps_set = [10, 25, 50, 75, 100, 200, 250, 300, 500, 750, 1000]\n",
    "kappa = 300\n",
    "summary_statistics = generate_results(n_steps_set, kappa, summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88daa8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_steps_set = [10, 25, 50, 75, 100, 200, 250, 300, 500, 750, 1000]\n",
    "kappa = 400\n",
    "summary_statistics = generate_results(n_steps_set, kappa, summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b2f82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_steps_set = [10, 25, 50, 75, 100, 200, 250, 300, 500, 750, 1000]\n",
    "kappa = 500\n",
    "summary_statistics = generate_results(n_steps_set, kappa, summary_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# creat results in terms of pandas datafreame and write them in csv file.\n",
    "#\n",
    "df_summary = pd.DataFrame(data=summary_statistics,\n",
    "                          columns=[\"kappa\", \"n_steps\",\n",
    "                                   \"entropy_avg\",\"entropy_std\",\"entropy_lower\",\"entropy_upper\",\n",
    "                                   \"efficiency_avg\",\"efficiency_std\",\"efficiency_lower\",\"efficiency_upper\",\n",
    "                                   \"straightness_avg\",\"straightness_std\",\"straightness_lower\",\"straightness_upper\",\n",
    "                                   \"d_net\", \"d_path\"])\n",
    "#\n",
    "csv_file_name = \"summary_statistics_CRW.csv\"\n",
    "df_summary.to_csv(csv_file_name, index=False, float_format='%.3f') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
